{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7876eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "import torch\n",
    "dataset = torch.load(\"../torchfem_dataset/simple_beam/combined.pt\",weights_only=False)\n",
    "#dataset_2 = torch.load(\"../torchfem_dataset/tube_2/tube_combined.pt\",weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1a350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process dataset for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def build_laststep_features(data, dtype=torch.float32):\n",
    "    ## Collapse to peak load timestep\n",
    "    #print(data.keys)\n",
    "    len = data['nodes'].f_ext.shape[0]\n",
    "    t = int(len/2-1)\n",
    "    # Nodes: x = [pos, bc, f_ext[-1], f_int[-1]]  (no leakage of u_ts into x)\n",
    "    #pos   = data['nodes'].pos.to(dtype) #no need to include\n",
    "    bc    = data['nodes'].bc.to(dtype)\n",
    "    f_ext = torch.Tensor(data['nodes'].f_ext[t]).to(dtype)\n",
    "    data['nodes'].fext = torch.Tensor(data['nodes'].f_ext[t]).to(dtype)\n",
    "    data['nodes'].x = torch.cat([bc, f_ext], dim=-1)\n",
    "\n",
    "    # Elements: x = [material (float), s_ts[-1].flatten(9)]\n",
    "    mat = data['elements'].material\n",
    "    if not torch.is_floating_point(mat):\n",
    "        mat = mat.float()\n",
    "    data['elements'].x = mat#torch.cat([mat], dim=-1)\n",
    "\n",
    "    # Target: nodes â†’ u_ts[-1] (3D)\n",
    "    data['nodes'].y_u = data['nodes'].u_ts[t].to(dtype)\n",
    "    data['nodes'].y_fint = data['nodes'].f_ts[t].to(dtype)\n",
    "    \n",
    "    # Target: elements\n",
    "    data['elements'].y_s = data['elements'].s_ts[t].to(dtype).reshape(-1, 9)\n",
    "    #data['elements'].y_d = data['elements'].d_ts[t].to(dtype)\n",
    "\n",
    "    # (Optional) free large tensors you won't use further to save RAM/VRAM\n",
    "    del data['nodes'].pos, data['nodes'].bc, data['nodes'].u_ts, data['nodes'].f_ext, data['nodes'].f_int\n",
    "    del data['elements'].s_ts, data['elements'].d_ts, data['elements'].material, data['nodes'].f_ts\n",
    "\n",
    "    #print(data.keys)\n",
    "\n",
    "    return data\n",
    "\n",
    "class HeteroStandardScaler:\n",
    "    def __init__(self):\n",
    "        self.node_stats = {}\n",
    "        self.edge_stats = {}\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        node_x = torch.cat([d['nodes'].x[:,3:].float() for d in dataset],dim=0)\n",
    "        node_f = torch.cat([d['nodes'].y_fint.float() for d in dataset],dim=0)\n",
    "        node_u = torch.cat([d['nodes'].y_u.float() for d in dataset],dim=0)\n",
    "        elem_s = torch.cat([d['elements'].y_s.float() for d in dataset],dim=0)\n",
    "        edge_acc = {}\n",
    "        # compute mean/std\n",
    "        for data in dataset:\n",
    "\n",
    "            for etype in data.edge_types:\n",
    "                if \"edge_attr\" in data[etype]:\n",
    "                    e = data[etype].edge_attr.float()\n",
    "                    self.edge_stats[etype] = {\n",
    "                        \"mean\": e.mean(dim=0, keepdim=True),\n",
    "                        \"std\":  e.std(dim=0, keepdim=True) + 1e-8}\n",
    "        \n",
    "        self.node_stats['nodes_x'] = {\n",
    "            \"mean\": node_x.mean(dim=0, keepdim=True),\n",
    "            \"std\":  node_x.std(dim=0, keepdim=True) + 1e-8}\n",
    "        self.node_stats['nodes_f'] = {\n",
    "            \"mean\": node_f.mean(dim=0, keepdim=True),\n",
    "            \"std\":  node_f.std(dim=0, keepdim=True) + 1e-8}\n",
    "        self.node_stats['nodes_u'] = {\n",
    "            \"mean\": node_u.mean(dim=0, keepdim=True),\n",
    "            \"std\":  node_u.std(dim=0, keepdim=True) + 1e-8}\n",
    "        self.node_stats['elem_s'] = {\n",
    "            \"mean\": elem_s.mean(dim=0, keepdim=True),\n",
    "            \"std\":  elem_s.std(dim=0, keepdim=True) + 1e-8}\n",
    "        for etype, mats in edge_acc.items():\n",
    "            E = torch.cat(mats, dim=0)\n",
    "            self.edge_stats[etype] = {\n",
    "                \"mean\": E.mean(dim=0, keepdim=True),\n",
    "                \"std\":  E.std(dim=0, keepdim=True) + 1e-8}\n",
    "\n",
    "\n",
    "    def transform(self, data: HeteroData):\n",
    "        # apply normalization\n",
    "        x = data['nodes'].x[:,3:].float()\n",
    "        m_x = self.node_stats['nodes_x'][\"mean\"]\n",
    "        s_x = self.node_stats['nodes_x'][\"std\"]\n",
    "        data['nodes'].x[:,3:] = (x - m_x) / s_x\n",
    "        y_f = data['nodes'].y_fint.float()\n",
    "        m_f = self.node_stats['nodes_f'][\"mean\"]\n",
    "        s_f = self.node_stats['nodes_f'][\"std\"]\n",
    "        data['nodes'].y_fint = (y_f - m_f) / s_f\n",
    "        y_u = data['nodes'].y_u.float()\n",
    "        m_u = self.node_stats['nodes_u'][\"mean\"]\n",
    "        s_u = self.node_stats['nodes_u'][\"std\"]\n",
    "        data['nodes'].y_u = (y_u - m_u) / s_u\n",
    "        y_s = data['elements'].y_s.float()\n",
    "        m_s = self.node_stats['elem_s'][\"mean\"]\n",
    "        s_s = self.node_stats['elem_s'][\"std\"]\n",
    "        data['elements'].y_s = (y_s - m_s) / s_s\n",
    "\n",
    "        # edges\n",
    "        for etype in data.edge_types:\n",
    "            if etype in self.edge_stats and \"edge_attr\" in data[etype]:\n",
    "                e = data[etype].edge_attr.float()\n",
    "                m = self.edge_stats[etype][\"mean\"]\n",
    "                s = self.edge_stats[etype][\"std\"]\n",
    "                data[etype].edge_attr = (e - m) / s\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def inverse_transform(self, data: HeteroData, fields=(\"nodes_x\",\"nodes_f\",\"nodes_u\",\"elem_s\")):\n",
    "        # Nodes\n",
    "        if \"nodes_x\" in fields:\n",
    "            m, s = self.node_stats[\"nodes_x\"][\"mean\"], self.node_stats[\"nodes_x\"][\"std\"]\n",
    "            data[\"nodes\"].x[:, 3:] = data[\"nodes\"].x[:, 3:] * s + m\n",
    "        if \"nodes_f\" in fields:\n",
    "            m, s = self.node_stats[\"nodes_f\"][\"mean\"], self.node_stats[\"nodes_f\"][\"std\"]\n",
    "            data[\"nodes\"].y_fint = data[\"nodes\"].y_fint * s + m\n",
    "        if \"nodes_u\" in fields:\n",
    "            m, s = self.node_stats[\"nodes_u\"][\"mean\"], self.node_stats[\"nodes_u\"][\"std\"]\n",
    "            data[\"nodes\"].y_u = data[\"nodes\"].y_u * s + m\n",
    "        if \"elem_s\" in fields:\n",
    "            m, s = self.node_stats[\"elem_s\"][\"mean\"], self.node_stats[\"elem_s\"][\"std\"]\n",
    "            data[\"elements\"].y_s = data[\"elements\"].y_s * s + m\n",
    "\n",
    "        # Edges\n",
    "        for etype in data.edge_types:\n",
    "            if etype in self.edge_stats and \"edge_attr\" in data[etype]:\n",
    "                m, s = self.edge_stats[etype][\"mean\"], self.edge_stats[etype][\"std\"]\n",
    "                data[etype].edge_attr = data[etype].edge_attr * s + m\n",
    "        return data\n",
    "\n",
    "scaler = HeteroStandardScaler()\n",
    "\n",
    "## DataLoader with train/val split\n",
    "dataset_p = [build_laststep_features(d) for d in dataset]\n",
    "scaler.fit(dataset_p)\n",
    "dataset_t = [scaler.transform(d) for d in dataset_p]\n",
    "\n",
    "def split_dataset(dataset, val_ratio=0.1, shuffle=True):\n",
    "    n = len(dataset)\n",
    "    idx = torch.randperm(n) if shuffle else torch.arange(n)\n",
    "    n_val = max(1, int(n * val_ratio))\n",
    "    val_idx = idx[:n_val].tolist()\n",
    "    train_idx = idx[n_val:].tolist()\n",
    "    train_set = [dataset[i] for i in train_idx]\n",
    "    val_set   = [dataset[i] for i in val_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "# preprocess first\n",
    "train_set, val_set = split_dataset(dataset_t, val_ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e07659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  nodes={\n",
      "    fext=[37500, 3],\n",
      "    x=[37500, 6],\n",
      "    y_u=[37500, 3],\n",
      "    y_fint=[37500, 3],\n",
      "  },\n",
      "  elements={\n",
      "    num_nodes=24552,\n",
      "    x=[24552, 1],\n",
      "    y_s=[24552, 9],\n",
      "  },\n",
      "  (elements, contributes, nodes)={\n",
      "    edge_index=[2, 196416],\n",
      "    edge_attr=[196416, 4],\n",
      "  },\n",
      "  (nodes, belongs_to, elements)={\n",
      "    edge_index=[2, 196416],\n",
      "    edge_attr=[196416, 4],\n",
      "  },\n",
      "  (nodes, adjacent, nodes)={\n",
      "    edge_index=[2, 99325],\n",
      "    edge_attr=[99325, 4],\n",
      "  },\n",
      "  (nodes, adjacent_rev, nodes)={\n",
      "    edge_index=[2, 99325],\n",
      "    edge_attr=[99325, 4],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = split_dataset(dataset_t, val_ratio=0.1)\n",
    "print(train_set[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ae5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1,-1),hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1,-1),out_channels)\n",
    "\n",
    "    def forward(self,x,edge_index):\n",
    "        x = torch.relu(self.conv1(x,edge_index))\n",
    "        x = self.conv2(x,edge_index)\n",
    "        return x\n",
    "    \n",
    "model = GNN(hidden_channels=64,out_channels=3)\n",
    "model = to_hetero(model,dataset_t[0].metadata(),aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "## Losses\n",
    "ALPHA_FINT = 0.3   # node f_int aux\n",
    "BETA_S     = 0.0   # element stress aux\n",
    "#GAMMA_D    = 0.0   # element damage aux (used only if available)\n",
    "LAMBDA_EQ  = 0.0   # equilibrium regularizer on nodes\n",
    "\n",
    "def compute_losses(batch, pred):\n",
    "    # Nodes\n",
    "    y_u    = batch['nodes'].y_u\n",
    "    #y_fint = batch['nodes'].y_fint\n",
    "    #fext   = batch['nodes'].x[:,3:6]\n",
    "    #bc = batch['nodes'].x[:,:3]\n",
    "    pu,  = pred['u'] \n",
    "    pf = pred['fint']\n",
    "\n",
    "    L_u    = F.mse_loss(pu, y_u)\n",
    "    #L_fint = F.mse_loss(pf, y_fint)\n",
    "    #L_eq   = F.mse_loss(((torch.ones_like(bc)-bc)*(pf - fext)).sum(),torch.zeros((),device=bc.device))\n",
    "\n",
    "\n",
    "    # Elements\n",
    "    ps = pred['s']\n",
    "    ys = batch['elements'].y_s\n",
    "    L_s = F.mse_loss(ps, ys)\n",
    "\n",
    "    # Optional damage\n",
    "    #L_d = torch.tensor(0.0, device=pu.device)\n",
    "    #if batch['elements'].y_d is not None:\n",
    "    #    yd = batch['elements'].y_d\n",
    "    #    pd = pred['d']\n",
    "    #    L_d = F.mse_loss(pd, yd)\n",
    "\n",
    "    loss = L_u + ALPHA_FINT * L_fint + LAMBDA_EQ * L_eq + BETA_S * L_s #+ GAMMA_D * L_d\n",
    "    return loss, {'L_u': L_u.item(), 'L_fint': L_fint.item(), 'L_eq': L_eq.item(),\n",
    "                  'L_s': L_s.item()}#, 'L_d': L_d.item()}\n",
    "\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total = 0.0; n = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        #pooled, cluster, node_type = graclus_pool_hetero(batch)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        #pred_pooled = model(pooled.x_dict, pooled.edge_index_dict)\n",
    "        #pred_unpooled = unpool_preds(pred_pooled, cluster, node_type, batch.node_types)\n",
    "        pred = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss, loss_dict = compute_losses(batch, pred)\n",
    "        #print(loss_dict)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        total += loss.item(); n += 1\n",
    "    return total / max(1, n), loss_dict\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "best_val = float('inf')\n",
    "best_state = None\n",
    "\n",
    "losses = [('train_loss','val_loss')]\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr, ld = run_epoch(train_loader, train=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va, ld = run_epoch(val_loader, train=False)\n",
    "    scheduler.step(va)\n",
    "    losses.append((ld))\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train {tr:.6f} | val {va:.6f}\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "torch.save(model.state_dict(), \"1123_sequential.pt\")\n",
    "print(\"Best val:\", best_val)\n",
    "\n",
    "loss_df = pd.DataFrame(losses)\n",
    "loss_df.to_csv('loss_1123_sequential.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ba478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): ModuleDict(\n",
       "    (elements__contributes__nodes): SAGEConv((-1, -1), 64, aggr=mean)\n",
       "    (nodes__belongs_to__elements): SAGEConv((-1, -1), 64, aggr=mean)\n",
       "    (nodes__adjacent__nodes): SAGEConv((-1, -1), 64, aggr=mean)\n",
       "    (nodes__adjacent_rev__nodes): SAGEConv((-1, -1), 64, aggr=mean)\n",
       "  )\n",
       "  (conv2): ModuleDict(\n",
       "    (elements__contributes__nodes): SAGEConv((-1, -1), 3, aggr=mean)\n",
       "    (nodes__belongs_to__elements): SAGEConv((-1, -1), 3, aggr=mean)\n",
       "    (nodes__adjacent__nodes): SAGEConv((-1, -1), 3, aggr=mean)\n",
       "    (nodes__adjacent_rev__nodes): SAGEConv((-1, -1), 3, aggr=mean)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0332958",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_set:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
